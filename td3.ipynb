{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA38jtUgtZsG"
      },
      "outputs": [],
      "source": [
        "# overall training structure based on http://bicmr.pku.edu.cn/~wenzw/bigdata/lect-dyna3w.pdf\n",
        "# policy and agents based on https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93, under MIT license\n",
        "\n",
        "%%capture\n",
        "!apt update\n",
        "!pip install 'gym[box2d]'\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "import copy\n",
        "from collections import deque\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 25 # videos can take a very long time to render so only do it every N episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb3-h1eGTSNg",
        "outputId": "375c21e4-7225-4ed8-d538-74a59287c838"
      },
      "outputs": [],
      "source": [
        "# optional Google drive integration - this will allow you to save and resume training, and may speed up redownloading the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# import os\n",
        "# os.mkdir('drive/MyDrive/TD3-Outputs/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMd9woqqDNIb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "EXPLORE_POLICY = 0.1\n",
        "LEARN_RATE = .001\n",
        "POLICY_DELAY = 2\n",
        "TAU = 0.005\n",
        "NOISE_POLICY = 0.2\n",
        "NOISE_CLIP = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHtclV_30Re"
      },
      "source": [
        "**Reinforcement learning agent**\n",
        "\n",
        "Replace this with your own agent - I recommend starting with TD3 (lecture 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXNHP8_U-rn"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_actions):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        # 3-layer linear nn\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        self.max_act = max_actions\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_act * torch.tanh(self.l3(x))\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        # initialise q1 and q2 networks\n",
        "        super(Critic, self).__init__()\n",
        "        # Q1..\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1) \n",
        "        # Q2..\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l5 = nn.Linear(400, 300)\n",
        "        self.l6 = nn.Linear(300, 1) \n",
        "        \n",
        "    def forward(self, s, a): \n",
        "        sa = torch.cat([s, a], 1) # state action pair\n",
        "        # Q1..\n",
        "        c1 = F.relu(self.l1(sa))\n",
        "        c1 = F.relu(self.l2(c1))\n",
        "        c1 = self.l3(c1)\n",
        "        # Q2..\n",
        "        c2 = F.relu(self.l4(sa))\n",
        "        c2 = F.relu(self.l5(c2))\n",
        "        c2 = self.l6(c2)\n",
        "        # return both results so smaller can be used\n",
        "        return (c1, c2)\n",
        "\n",
        "class TD3():\n",
        "    # td3 (e.g. twin ddpg) agent, ready to be instanciated into a policy\n",
        "    def __init__(self, state_dim, action_dim, max_action, env, device):\n",
        "        super(TD3, self).__init__()\n",
        "\n",
        "        # call actor network...\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=LEARN_RATE)\n",
        "        self.device = device\n",
        "\n",
        "        # call critic network...\n",
        "        self.critic = Critic(state_dim, action_dim).to(device) # only needs state + action\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=LEARN_RATE)\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state, noise=0.1): \n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten() # determines next action based on current state and policy model\n",
        "        if(noise == EXPLORE_POLICY): \n",
        "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0])) # adds some noise from distribution to policy\n",
        "\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), 'drive/MyDrive/TD3-Outputs/td3_actor_finalized.pth')\n",
        "        torch.save(self.critic.state_dict(), 'drive/MyDrive/TD3-Outputs/td3_critic_finalized.pth')\n",
        "        return\n",
        "    \n",
        "    def load(self):\n",
        "        self.actor.load_state_dict(torch.load('./td3_actor_finalized.pth',  map_location=torch.device('cpu')))\n",
        "        self.critic.load_state_dict(torch.load('./td3_critic_finalized.pth',  map_location=torch.device('cpu')))\n",
        "        return\n",
        "\n",
        "    def train(self, replay_buffer, current_iteration): \n",
        "        state, action, reward, next_state, done = replay_buffer.sample() # sample random transitions\n",
        "        # to improve: implement a prioritised replay buffer so not entirely stochastic\n",
        "        \n",
        "        tensor_cpy = action.clone().detach()\n",
        "        noise = tensor_cpy.normal_(0, NOISE_POLICY).clamp(-NOISE_CLIP, NOISE_CLIP) # adds some noise\n",
        "      \n",
        "        next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action) # clips action and noise sum within +/- maximum\n",
        "        \n",
        "        # Which Qs are we aiming for?:\n",
        "        target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "        target_q = ((torch.min(target_q1, target_q2)) * (1-done)) + reward\n",
        "        curr_q1, curr_q2 = self.critic(state, action)\n",
        "\n",
        "        # learn with MSE loss regression\n",
        "        critic_loss = F.mse_loss(curr_q1, target_q) + F.mse_loss(curr_q2, target_q)\n",
        "        self.critic_optimizer.zero_grad() # ignore any previously learnt gradients here\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        if (current_iteration % POLICY_DELAY == 0): # one in two actions\n",
        "            \n",
        "            actor_loss = -self.critic(state, self.actor(state))[0].mean() # gradient ascent based on critic output\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):    \n",
        "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-9vv6BXYBMU"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay:\n",
        "    # a memory buffer to store previous state / action experience for future random sampling\n",
        "    def __init__(self, buffer_size, batch_size, device):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size= batch_size\n",
        "        self.device = device\n",
        "        self.ptr = 0\n",
        "\n",
        "    def __len__(self): # rewrite magic\n",
        "        return len(self.buffer)\n",
        "\n",
        "    # Add a transition to the memory\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        if self.ptr < self.buffer.maxlen:         # is buffer full yet?\n",
        "            self.buffer.append((state, action, reward, new_state, done))\n",
        "        else: \n",
        "            self.buffer[int(self.ptr)] = (state, action, reward, new_state, done)\n",
        "            self.ptr = (self.ptr + 1) % self.buffer.maxlen\n",
        "\n",
        "    # sample memory\n",
        "    def sample(self):\n",
        "        sample = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*sample)\n",
        "        states = torch.from_numpy(np.array(states, dtype=np.float32)).to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions, dtype=np.float32)).to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards, dtype=np.float32).reshape(-1, 1)).to(self.device)\n",
        "        next_states = torch.from_numpy(np.array(next_states, dtype=np.float32)).to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones, dtype=np.uint8).reshape(-1, 1)).float().to(self.device)\n",
        "        return (states, actions, rewards, next_states, dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "# env = gym.make(\"Pendulum-v0\") # useful continuous environment for quick experiments\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this if your agent consistently aces BipedalWalker-v3\n",
        "env = gym.wrappers.Monitor(env, \"drive/MyDrive/TD3-Outputs/video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUw4h980jfnu",
        "outputId": "911ff41a-b035-43d2-ae41-1bb04f7f8308"
      },
      "outputs": [],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rDl6ViIDlVOk",
        "outputId": "56c1e1be-b544-4249-e8f0-8a6ba77a4dc0"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import take\n",
        "# in the submission please use seed 42 for verification\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"drive/MyDrive/TD3-Outputs/agent-log.txt\",\"w+\")\n",
        "\n",
        "# aditional variables\n",
        "buffer_size = 1000000\n",
        "batch_size = 100\n",
        "noise = 0.1\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "# initialise agent\n",
        "agent = TD3(obs_dim, act_dim, max_action, env, device)\n",
        "\n",
        "\n",
        "agent.load()\n",
        "buffer = ExperienceReplay(buffer_size, batch_size, device)\n",
        "# per = PrioritsedExperienceReplay(buffer_size)\n",
        "\n",
        "save_score = 400\n",
        "max_episodes = 10000\n",
        "max_timesteps = 2000\n",
        "explore_timesteps_1 = 1000\n",
        "explore_timesteps_2 = 1500\n",
        "\n",
        "best_reward = -999999999999999999\n",
        "scores_over_episodes = []\n",
        "\n",
        "state = env.reset()\n",
        "# training procedure:\n",
        "for episode in range(1, max_episodes+1):\n",
        "    if episode < 50:\n",
        "      # fast exploration phase, builds up replay buffer\n",
        "        T = explore_timesteps_1\n",
        "    elif episode < 100:\n",
        "        T = explore_timesteps_2\n",
        "    else:\n",
        "        T = max_timesteps\n",
        "    ep_reward = 0\n",
        "    # state = env.reset()\n",
        "    for t in range(T):\n",
        "\n",
        "        # select the agent action\n",
        "        action = agent.select_action(state) + np.random.normal(0, max_action * noise, size=act_dim)\n",
        "        action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # take action in environment and get r and s'\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        buffer.store_transition(state, action, reward, next_state, done)\n",
        "        # per.add((state,action,reward,next_state,done),reward)\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "\n",
        "        if len(buffer) > batch_size:\n",
        "        # if len(per) > batch_size:\n",
        "            agent.train(buffer, t)\n",
        "            # agent.train(per, t)\n",
        "        \n",
        "        # stop iterating when the episode finished\n",
        "        if done or t>=(T-1):\n",
        "            scores_over_episodes.append(ep_reward)\n",
        "            try: # tries in case episode is not over\n",
        "                state = env.reset()\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    if(np.mean(scores_over_episodes[-50:]) > save_score):\n",
        "        print('Saving agent- past 50 scores gave better avg than ', save_score)\n",
        "        best_reward = np.mean(scores_over_episodes[-50:])\n",
        "        save_score = best_reward\n",
        "        agent.save()\n",
        "        break # Saved agent. Break out of episodes and end, 400 is pretty good. \n",
        "\n",
        "    if(episode >= 0 and ep_reward > best_reward):\n",
        "        # print('Saving agent- score for this episode was better than best-known score..')\n",
        "        best_reward = ep_reward\n",
        "        agent.save() # Save current policy + optimizer\n",
        "\n",
        "    # append the episode reward to the reward list\n",
        "    reward_list.append(ep_reward)\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "    log_f.flush()\n",
        "    ep_reward = 0\n",
        "\n",
        "    \n",
        "    \n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OI_oN-nxAi8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hkIZhoExAnd"
      },
      "source": [
        "# Unnsuccesful attempt at PER...\n",
        "#### Retained for reference / proof of experimentation attempts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr2YsMUQxMnU"
      },
      "outputs": [],
      "source": [
        "# below code based on https://github.com/djmax008/GEIRINA_baseline/blob/master/prioritized_memory.py, under MIT license\n",
        "\n",
        "class SumTree():\n",
        "\n",
        "    data_pointer = 0\n",
        "    data_length = 0\n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "\n",
        "        self.capacity = int(capacity)\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_length\n",
        "    \n",
        "    def add(self, data, priority):\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data\n",
        "        self.update (tree_index, priority)\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:\n",
        "            self.data_pointer = 0\n",
        "        if self.data_length < self.capacity:\n",
        "            self.data_length += 1\n",
        "    \n",
        "    def update(self, tree_index, priority):\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        \n",
        "        while tree_index != 0:\n",
        "            tree_index = (tree_index - 1) // 2\n",
        "            self.tree[tree_index] += change    \n",
        "    \n",
        "    def get_leaf(self, v):\n",
        "        parent_index = 0\n",
        "        \n",
        "        while True: \n",
        "            left_child_index = 2 * parent_index + 1\n",
        "            right_child_index = left_child_index + 1\n",
        "            \n",
        "            if left_child_index >= len(self.tree):\n",
        "                leaf_index = parent_index\n",
        "                break\n",
        "            else: \n",
        "                if v <= self.tree[left_child_index]:\n",
        "                    parent_index = left_child_index\n",
        "                else:\n",
        "                    v -= self.tree[left_child_index]\n",
        "                    parent_index = right_child_index\n",
        "        \n",
        "        data_index = leaf_index - self.capacity + 1\n",
        "        \n",
        "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
        "    \n",
        "    @property\n",
        "    def total_priority(self):\n",
        "        return self.tree[0]  \n",
        "\n",
        "class PER():  \n",
        "    epsilon = 0.01 \n",
        "    alpha = 0.6 \n",
        "    beta = 0.4  \n",
        "    beta_increment_per_sampling = 1e-4 \n",
        "    absolute_error_upper = 1.  \n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tree)\n",
        "    \n",
        "    def is_full(self):\n",
        "        return len(self.tree) >= self.tree.capacity\n",
        "    \n",
        "    def add(self, sample, error = None):\n",
        "        if error is None:\n",
        "            priority = np.amax(self.tree.tree[-self.tree.capacity:])\n",
        "            if priority == 0: priority = self.absolute_error_upper\n",
        "        else:\n",
        "            priority = min((abs(error) + self.epsilon) ** self.alpha, self.absolute_error_upper)\n",
        "        self.tree.add(sample, priority)\n",
        "    \n",
        "    def sample(self, n):\n",
        "\n",
        "        minibatch = []\n",
        "        \n",
        "        idxs = np.empty((n,), dtype=np.int32)\n",
        "        is_weights = np.empty((n,), dtype=np.float32)\n",
        "\n",
        "        priority_segment = self.tree.total_priority / n    \n",
        "        \n",
        "        self.beta = np.amin([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
        "        \n",
        "        p_min = np.amin(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
        "        max_weight = (p_min * n) ** (-self.beta)\n",
        "        \n",
        "        for i in range(n):\n",
        "\n",
        "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
        "            value = np.random.uniform(a, b)\n",
        "            \n",
        "\n",
        "            index, priority, data = self.tree.get_leaf(value)\n",
        "            \n",
        "            sampling_probabilities = priority / self.tree.total_priority\n",
        "            is_weights[i] = np.power(n * sampling_probabilities, -self.beta)/ max_weight\n",
        "            \n",
        "            idxs[i]= index\n",
        "            minibatch.append(data)\n",
        "            \n",
        "        return idxs, minibatch, is_weights\n",
        "    \n",
        "    def batch_update(self, idxs, errors):\n",
        "\n",
        "        errors = errors + self.epsilon\n",
        "        clipped_errors = np.minimum(errors, self.absolute_error_upper)\n",
        "        ps = np.power(clipped_errors, self.alpha)\n",
        "        \n",
        "        for idx, p in zip(idxs, ps):\n",
        "            self.tree.update(idx, p)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RL-Assignment",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
